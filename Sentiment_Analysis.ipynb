{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeO-hSVi3NBs"
      },
      "source": [
        "**SENTIMENT ANALYSIS **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rsco0tSz3Zeo"
      },
      "source": [
        "LOADING THE DATA AND CHECKING IT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHAzJLJV1TGz",
        "outputId": "917d7036-1839-49cd-da8e-f5da78520e9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  sentiment          id                          date     query  \\\n",
            "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
            "1         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
            "2         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
            "3         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "4         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "\n",
            "          username                                               text  \n",
            "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
            "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
            "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
            "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
            "4           Karoli  @nationwideclass no, it's not behaving at all....  \n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize empty lists to store the data\n",
        "data = []\n",
        "\n",
        "# Open the CSV file with Python's csv module\n",
        "with open('Sentimen140.csv', 'r', encoding='latin-1') as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "        if len(row) == 6:  # Ensure only rows with 6 columns are processed\n",
        "            data.append(row)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data, columns=['sentiment', 'id', 'date', 'query', 'username', 'text'])\n",
        "\n",
        "# Print the first few rows\n",
        "print(df.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2AHTpVf7SGv",
        "outputId": "6f83af17-7d6e-4afe-e0a1-c1365f00cde8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  sentiment          id                          date     query  \\\n",
            "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
            "1         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
            "2         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
            "3         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "4         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "\n",
            "          username                                               text  \n",
            "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
            "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
            "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
            "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
            "4           Karoli  @nationwideclass no, it's not behaving at all....  \n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "data = []\n",
        "\n",
        "\n",
        "with open('Sentimen140.csv', 'r', encoding='latin-1') as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "\n",
        "        if len(row) == 6:\n",
        "            data.append(row)\n",
        "\n",
        "df = pd.DataFrame(data, columns=['sentiment', 'id', 'date', 'query', 'username', 'text'])\n",
        "\n",
        "print(df.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4lCIwYR7_sL",
        "outputId": "ed3ecb6f-af08-4147-e7b0-5f3375a99dbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  sentiment          id                          date     query  \\\n",
            "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
            "1         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
            "2         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
            "3         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "4         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "\n",
            "          username                                               text  \n",
            "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
            "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
            "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
            "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
            "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
            "sentiment\n",
            "0    800000\n",
            "4    800000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df.head())\n",
        "print(df['sentiment'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX8N4sv68aDC"
      },
      "source": [
        "DATA PREPROCESSING - TEXT PREPROCESSING USING NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj6zzdYG8Zsi",
        "outputId": "429da011-e82b-44a1-aeb3-35bfe4a28702"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text  \\\n",
            "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
            "1  is upset that he can't update his Facebook by ...   \n",
            "2  @Kenichan I dived many times for the ball. Man...   \n",
            "3    my whole body feels itchy and like its on fire    \n",
            "4  @nationwideclass no, it's not behaving at all....   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0      thats bummer shoulda got david carr third day  \n",
            "1  upset cant update facebook texting might cry r...  \n",
            "2    dived many time ball managed save rest go bound  \n",
            "3                    whole body feel itchy like fire  \n",
            "4                           behaving im mad cant see  \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK resources (only needs to be done once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function to preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "\n",
        "    # Remove mentions (@username)\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)\n",
        "\n",
        "    # Remove special characters and numbers, keeping only alphabets\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "\n",
        "    # Tokenize the text (split it into individual words)\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords (common words that don't contribute to sentiment)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatize tokens (convert words to their base forms)\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Join tokens back into a single string\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Apply the preprocessing function to the 'text' column\n",
        "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Display the original and cleaned text for comparison\n",
        "print(df[['text', 'cleaned_text']].head())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ycc_wC6DHxa"
      },
      "source": [
        "NOW WE WILL VECTORIZE THE TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bcfGaj3XDMqB",
        "outputId": "c64ceaf5-6aa4-47ae-c44b-2716ce5c0399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF matrix shape: (1600000, 405204)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the cleaned text data into TF-IDF features\n",
        "X = tfidf.fit_transform(df['cleaned_text'])\n",
        "\n",
        "# Check the shape of the resulting feature matrix\n",
        "print(\"TF-IDF matrix shape:\", X.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKAoN4egEQGf"
      },
      "source": [
        "NOW WE ARE READY WITH THE DATA . WE WILL SPLIT THE DATA INTO TRAINING AND TEST DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QE9Vam94Ehk-",
        "outputId": "36c2cbd3-f739-4c65-f62a-016369544e7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (1248000, 405204)\n",
            "Testing data shape: (352000, 405204)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# X contains the TF-IDF features, and y contains the sentiment labels\n",
        "X = tfidf.fit_transform(df['cleaned_text'])\n",
        "\n",
        "# Here, we're using the sentiment labels for classification (you can change this if you're using binary sentiment)\n",
        "y = df['sentiment']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=42)\n",
        "\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Testing data shape:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY7J9bBuF4d5"
      },
      "source": [
        "NOW WE WILL USE LOGISTIC REGRESSION MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvvgJbRbF7V-",
        "outputId": "3c762c94-5101-4ff6-cddf-3bd5f04b8299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Accuracy: 0.7818039772727273\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.76      0.78    175531\n",
            "           4       0.77      0.80      0.79    176469\n",
            "\n",
            "    accuracy                           0.78    352000\n",
            "   macro avg       0.78      0.78      0.78    352000\n",
            "weighted avg       0.78      0.78      0.78    352000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9H3sMq8F75g"
      },
      "source": [
        "LET US HYPERTUNE PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZWqmLnTF26L",
        "outputId": "263da229-c32a-4edf-bde9-eb6ba14df914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "[CV] END ...............................C=0.01, solver=lbfgs; total time=   8.8s\n",
            "[CV] END ...............................C=0.01, solver=lbfgs; total time=   5.8s\n",
            "[CV] END ...............................C=0.01, solver=lbfgs; total time=   8.1s\n",
            "[CV] END ...............................C=0.01, solver=lbfgs; total time=   6.0s\n",
            "[CV] END ...............................C=0.01, solver=lbfgs; total time=   7.2s\n",
            "[CV] END ...........................C=0.01, solver=liblinear; total time=   7.8s\n",
            "[CV] END ...........................C=0.01, solver=liblinear; total time=   9.1s\n",
            "[CV] END ...........................C=0.01, solver=liblinear; total time=   7.0s\n",
            "[CV] END ...........................C=0.01, solver=liblinear; total time=   9.1s\n",
            "[CV] END ...........................C=0.01, solver=liblinear; total time=   7.3s\n",
            "[CV] END ................................C=0.1, solver=lbfgs; total time=  18.0s\n",
            "[CV] END ................................C=0.1, solver=lbfgs; total time=   9.1s\n",
            "[CV] END ................................C=0.1, solver=lbfgs; total time=  12.8s\n",
            "[CV] END ................................C=0.1, solver=lbfgs; total time=  13.2s\n",
            "[CV] END ................................C=0.1, solver=lbfgs; total time=   9.5s\n",
            "[CV] END ............................C=0.1, solver=liblinear; total time=  15.3s\n",
            "[CV] END ............................C=0.1, solver=liblinear; total time=  13.9s\n",
            "[CV] END ............................C=0.1, solver=liblinear; total time=  14.0s\n",
            "[CV] END ............................C=0.1, solver=liblinear; total time=  15.0s\n",
            "[CV] END ............................C=0.1, solver=liblinear; total time=  15.5s\n",
            "[CV] END ..................................C=1, solver=lbfgs; total time=  42.9s\n",
            "[CV] END ..................................C=1, solver=lbfgs; total time=  15.9s\n",
            "[CV] END ..................................C=1, solver=lbfgs; total time=  28.1s\n",
            "[CV] END ..................................C=1, solver=lbfgs; total time=  16.5s\n",
            "[CV] END ..................................C=1, solver=lbfgs; total time=  33.9s\n",
            "[CV] END ..............................C=1, solver=liblinear; total time=  33.8s\n",
            "[CV] END ..............................C=1, solver=liblinear; total time=  32.8s\n",
            "[CV] END ..............................C=1, solver=liblinear; total time=  33.8s\n",
            "[CV] END ..............................C=1, solver=liblinear; total time=  35.1s\n",
            "[CV] END ..............................C=1, solver=liblinear; total time=  33.5s\n",
            "[CV] END .................................C=10, solver=lbfgs; total time= 1.3min\n",
            "[CV] END .................................C=10, solver=lbfgs; total time= 1.1min\n",
            "[CV] END .................................C=10, solver=lbfgs; total time= 1.3min\n",
            "[CV] END .................................C=10, solver=lbfgs; total time=  41.8s\n",
            "[CV] END .................................C=10, solver=lbfgs; total time= 1.0min\n",
            "[CV] END .............................C=10, solver=liblinear; total time= 1.5min\n",
            "[CV] END .............................C=10, solver=liblinear; total time= 1.5min\n",
            "[CV] END .............................C=10, solver=liblinear; total time= 1.2min\n",
            "[CV] END .............................C=10, solver=liblinear; total time= 1.5min\n",
            "[CV] END .............................C=10, solver=liblinear; total time= 1.1min\n",
            "Best Hyperparameters: {'C': 1, 'solver': 'liblinear'}\n",
            "Improved Accuracy: 0.7828920454545455\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
        "    'solver': ['lbfgs', 'liblinear']  # Solver for optimization\n",
        "}\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=2)\n",
        "\n",
        "# Fit the model using grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Improved Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4d2oMHRYjk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd27d798-733f-4142-852f-52d2e98aaed6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the hyperparameter grid for SVM\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10, 100],  # Regularization strength\n",
        "    'kernel': ['linear', 'rbf', 'poly'],  # Different kernel types\n",
        "    'gamma': ['scale', 'auto']  # Kernel coefficient for 'rbf', 'poly'\n",
        "}\n",
        "\n",
        "# Initialize the SVM model\n",
        "svm_model = SVC()\n",
        "\n",
        "# Set up GridSearchCV for SVM\n",
        "grid_search_svm = GridSearchCV(estimator=svm_model, param_grid=param_grid_svm, cv=5, scoring='accuracy', verbose=2)\n",
        "\n",
        "# Fit the model using grid search\n",
        "grid_search_svm.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters for SVM\n",
        "print(\"Best Hyperparameters for SVM:\", grid_search_svm.best_params_)\n",
        "\n",
        "# Evaluate the best SVM model on the test set\n",
        "y_pred_svm = grid_search_svm.best_estimator_.predict(X_test)\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "print(\"SVM Accuracy after GridSearch:\", accuracy_svm)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}